# Contributing to Patient Assistant Avatar

We are excited to have you contribute to the **Patient Assistant Avatar** project! This guide will help you integrate the backend and frontend components to enable the avatar to "talk" by syncing audio and lip movements seamlessly.

---

## **Contribution Scope**
The primary goal is to integrate backend-generated audio and blendshape data into the frontend avatar so it can:  
1. Play audio responses generated by the backend.  
2. Sync the avatar’s lip movements using blendshape data for realistic speech animation.

---

## **Prerequisites**
Before contributing, ensure you have:  
1. Basic understanding of **React.js**, **Three.js**, and **React Three Fiber** for the frontend.  
2. Familiarity with **Flask**, **LangChain**, and **Microsoft Cognitive Services Speech SDK** for the backend.  
3. Installed all required dependencies for both frontend and backend. (Refer to the [README.md](README.md) for installation instructions.)

---

## **Steps to Enable Avatar Talking**
### **1. Backend: Preparing the Data**
The backend provides two essential outputs for the talking avatar:
- **Audio Response**: Base64-encoded audio file generated using Microsoft Cognitive Services Speech SDK.  
- **Blendshape Data**: JSON object representing blendshapes for lip-sync animation.

Ensure the backend is configured to return these in the response:
```json
{
  "text": "Your generated text response",
  "audio": "Base64EncodedAudioString",
  "blendshapes": {
    "eyeLookDownRight":0.2,
     "eyeLookInRight":0.33,
     "eyeLookOutRight":0.22,
   
  }
}
```

### **2. Frontend: Decoding and Integrating Audio**
Update the frontend to handle audio playback:
- Decode the Base64 audio string returned by the backend.
- Use the `Audio` API or a library like `howler.js` to play the decoded audio.

Example in `makeSpeech.jsx`:
```javascript
import axios from "axios";
import useSpeechStore from "./store/useSpeechStore";

const makeSpeech = async (text) => {
  const setAudio = useSpeechStore.getState().setAudio;
  const setBlendshapes = useSpeechStore.getState().setBlendshapes;
  const setSpeak = useSpeechStore.getState().setSpeak;

  try {
    const response = await axios.post("http://127.0.0.1:5000/generate", {
      input: text,
    });

    const { audio, blendshapes } = response.data;

    // Update Zustand store
    setAudio(audio);
    setBlendshapes(blendshapes);
    setSpeak(true); // Trigger the Avatar to speak
  } catch (error) {
    console.error("Error in makeSpeech:", error);
  }
};

export default makeSpeech;
```

### **3. Frontend: Syncing Blendshapes**
Use the blendshape data to animate the avatar:
1. Pass the blendshape data to the `Avatar.jsx` component.
2. Update the avatar's 3D model in real time using the blendshape weights.


### **4. Integrating API Calls**
Use `fetch` or `axios` to get data from the backend and pass it to the `playAudio` and `Avatar` components.

Example in `Chat.jsx`:
```javascript
import { playAudio } from './makeSpeech';

function handleResponse(response) {
  // Play audio
  playAudio(response.audio);

  // Pass blendshape data to the Avatar component
  setBlendshapes(response.blendshapes);
}

async function fetchChatResponse(message) {
  const res = await fetch('/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ query: message })
  });
  const data = await res.json();
  handleResponse(data);
}
```

---

## **Development Workflow**
### **1. Fork and Clone the Repository**
1. Fork the repository to your GitHub account.  
2. Clone your forked repository locally:
   ```bash
   git clone https://github.com/your-username/repository.git
   cd repository
   ```

### **2. Create a New Branch**
Create a new branch for your feature:
```bash
git checkout -b feature/avatar-talking
```

### **3. Make Changes**
Follow the steps outlined above to implement the integration.

### **4. Test Your Changes**
- Start the backend server:
  ```bash
  python app.py
  ```
- Start the frontend server:
  ```bash
  npm start
  ```
- Test the avatar’s speech and animation.

### **5. Commit and Push**
Once you’ve tested your changes, commit them:
```bash
git add .
git commit -m "Integrated avatar talking functionality"
git push origin feature/avatar-talking
```

### **6. Submit a Pull Request**
Submit a pull request to the `main` branch of the original repository. Provide a detailed description of the changes you made.

---

## **Guidelines**
1. Follow coding standards for React.js and Python.  
2. Document your code thoroughly with comments.  
3. Ensure compatibility across major browsers.  
4. Test all changes before submitting.

---

## **Support**
If you have any questions or encounter issues, feel free to open an issue in the repository or reach out directly to **Mohammed Bahageel**:  
- **Email**: [mohammed.bahageel@example.com](mailto:mohammed.bahageel@example.com)  
- **GitHub**: [github.com/mohammed-bahageel](https://github.com/mohammed-bahageel)

-